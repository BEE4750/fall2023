{
  "hash": "6c02ee18f207a04d9510ba25b3b87fca",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: \"Why Monte Carlo Works \"\nsubtitle: \"Lecture 11\"\nauthor: \"Vivek Srikrishnan\"\ncourse: \"BEE 4750\"\ninstitution: \"Cornell University\"\ndate: \"September 20, 2023\"\nformat:\n    revealjs:\n        slide-number: c/t\n        show-slide-number: all\n        center-title-slide: true\n        width: 1280\n        height: 720\n        transition: none\n        toc: true\n        toc-depth: 1\n        toc-title: \"Overview\"\n        history: false\n        link-external-newwindow: true\n        theme: ../sass/slides.scss\n        footer: \"[BEE 4750, Environmental Systems Analysis](https://viveks.me/environmental-systems-analysis)\"\n        template-partials:\n            - title-slide.html\n        menu:\n            numbers: true\n        html-math-method: mathjax\n        include-in-header: mathjax-config.html\n        date-format: long\n        highlight-style: tango\n        code-line-numbers: false\njulia:\n    exeflags: [\"+1.10.4\"]\nexecute:\n    freeze: auto\n---\n\n# Review and Questions\n\n## Monte Carlo Simulation\n\nMonte Carlo is **stochastic simulation**.\n\n```{dot}\n//| fig-width: 100%\ndigraph G {\n    graph [\n        rankdir=LR\n        layout=dot\n    ]\n    node [\n        fontname = \"IBM Plex Sans, sans-serif\"\n        fontsize=25\n    ]\n    edge [\n        arrowsize=0.75\n        labeldistance=3\n        penwidth=3\n        fontname = \"IBM Plex Sans, sans-serif\"\n        fontsize=25\n        style=dashed\n        color=\"#b31b1b\"\n        fontcolor=\"#b31b1b\"\n    ]\n    a [label=\"Probability\\n Distribution\"]\n    b [label = \"Random\\n Samples\"]\n    c [label=\"Model\"]\n    d [label=\"Outputs\"]\n\n    a -> b [\n        label=\"Sample\"\n    ]\n    b -> c [\n        label=\"Input\"\n    ]\n    c -> d [\n        label=\"Simulate\"\n    ]\n}\n\n```\n\n## Goals of Monte Carlo\n\nMonte Carlo is a broad method, which can be used to:\n\n1. Obtain probability distributions of outputs (***uncertainty propagation***);\n2. Estimate deterministic quantities (***Monte Carlo estimation***).\n\n## Monte Carlo Estimation\n\nMonte Carlo estimation involves framing the (deterministic) quantity of interest as a summary statistic of a random process.\n\n## Questions?\n\n:::: {.columns .center}\n::: {.column width=\"40%\"}\n![](images/vsrikrish-poll.png){fig-alt=\"Poll Everywhere QR Code\" fig-align=\"center\" width=\"100%\"}\n:::\n::: {.column width=60%}\n**Text**: VSRIKRISH to 22333\n\n**URL**: [https://pollev.com/vsrikrish](https://pollev.com/vsrikrish)\n<br><br>\n[See Results](https://www.polleverywhere.com/multiple_choice_polls/qKeig0hHShZYHAOmvZtZm?preview=true&controls=none){preview-link=\"true\"}\n:::\n::::\n\n\n# Monte Carlo: More Formally\n\n## Why Monte Carlo Works\n\nWe can formalize Monte Carlo estimation as the computation of the expected value of a random quantity $Y$, $\\mu = \\mathbb{E}[Y]$.\n\nTo do this, generate $n$ independent and identically distributed values $Y_1, \\ldots, Y_n$.  Then the sample estimate is\n\n$$\\tilde{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$$\n\n## Monte Carlo (Formally)\n\nMore generally, we want to compute some quantity $Y=f(X)$, where $X$ is distributed according to some probability distribution $p(x)$ and $f(x)$ is a real-valued function over a domain $D$.\n\nThen\n$$\\mu = \\mathbb{E}(Y) = \\int_D f(x)p(x)dx.$$\n\n## The Law of Large Numbers\n\nIf \n\n(1) $Y$ is a random variable and its expectation exists and \n\n(2) $Y_1, \\ldots, Y_n$ are independently and identically distributed\n\nThen by the **weak law of large numbers**:\n\n$$\\lim_{n \\to \\infty} \\mathbb{P}\\left(\\left|\\tilde{\\mu}_n - \\mu\\right| \\leq \\varepsilon \\right) = 1$$\n\n## The Law of Large Numbers\n\nIn other words, *eventually* Monte Carlo estimates will get within an arbitrary error of the true expectation. But how large is large enough?\n\nNote that the law of large numbers applies to vector-valued functions as well. The key is that $f(x) = Y$ just needs to be sufficiently well-behaved.\n\n## Monte Carlo Sample Mean\n\nThe sample mean $\\tilde{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$ is itself a random variable.\n\n::: {.fragment .fade-in}\nWith some assumptions (the mean of $Y$ exists and $Y$ has finite variance), the expected Monte Carlo sample mean $\\mathbb{E}[\\tilde{\\mu}_n]$ is\n\n$$\\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}[Y_i] = \\frac{1}{n} n \\mu = \\mu$$\n\nSo the Monte Carlo estimate is an *unbiased* estimate of the mean.\n:::\n\n## Monte Carlo Error\n\nWe'd like to know more about the error of this estimate for a given sample size. The variance of this estimator is\n\n$$\\tilde{\\sigma}_n^2 = \\text{Var}\\left(\\tilde{\\mu}_n\\right) = \\mathbb{E}\\left((\\tilde{\\mu}_n - \\mu)^2\\right) = \\frac{\\sigma_Y^2}{n}$$\n\n::: {.fragment .fade-in}\nSo as $n$ increases, the *standard error* decreases:\n\n$$\\tilde{\\sigma}_n = \\frac{\\sigma_Y}{\\sqrt{n}}$$\n:::\n\n## Monte Carlo Error\n\nIn other words, if we want to decrease the Monte Carlo error by 10x, we need 100x additional samples. **This is not an ideal method for high levels of accuracy.** \n\n::: {.fragment .fade-in}\n::: {.quote}\n> Monte Carlo is an extremely bad method. It should only be used when all alternative methods are worse.\n\n::: {.cite}\n--- Sokal, *Monte Carlo Methods in Statistical Mechanics*, 1996\n:::\n:::\n:::\n\n::: {.fragment .fade-in}\n\nBut...often most alternatives *are* worse!\n:::\n\n## When Might We Want to Use Monte Carlo?\n\n::: {.fragment .fade-in}\n- All models are wrong, and so there always exists some irreducible model error. Can we reduce the Monte Carlo error enough so it's less than the model error and other uncertainties?\n- We often need a lot of simulations. Do we have enough computational power?\n:::\n\n## When Might We Want to Use Monte Carlo?\n\nIf you can compute your answers analytically, you probably should. \n\nBut for *many* systems problems, this is either\n\n1. Not possible;\n2. Requires a lot of stylization and simplification.\n\n## Monte Carlo Confidence Intervals\n\nThis error estimate lets us compute confidence intervals for the MC estimate.\n\n## What is a Confidence Interval?\n\n**Remember**: an $\\alpha$-confidence interval is an interval such that $\\alpha \\%$ of intervals constructed after a given experiment will contain the true value.\n\n::: {.fragment .fade-in}\n\nIt is **not** an interval which contains the true value $\\alpha \\%$ of the time. This concept does not exist within frequentist statistics, and this mistake is often made.\n:::\n\n## How To Interpret Confidence Intervals\n\n:::: {.columns}\n::: {.column width=65%}\nTo understand confidence intervals, think of horseshoes! \n\nThe post is a fixed target, and my accuracy informs how confident I am that I will hit the target with any given toss.\n\n:::\n::: {.column width=35%}\n\n![Cartoon of horseshoes](https://www.wikihow.com/images/thumb/2/20/Throw-a-Horseshoe-Step-4-Version-4.jpg/aid448076-v4-728px-Throw-a-Horseshoe-Step-4-Version-4.jpg.webp)\n\n::: {.caption}\nSource: <https://www.wikihow.com/Throw-a-Horseshoe>\n:::\n:::\n::::\n\n## How To Interpret Confidence Intervals\n\n**But once I make the throw, I've either hit or missed.**\n\nThe confidence level $\\alpha\\%$ expresses the *pre-experimental* frequency by which a confidence interval will contain the true value. So for a 95% confidence interval, there is a 5% chance that a given sample was an outlier and the interval is inaccurate.\n\n\n## Monte Carlo Confidence Intervals\n\nOK, back to Monte Carlo...\n\n**Basic Idea**: The *Central Limit Theorem* says that with enough samples, the errors are normally distributed:\n\n$$\\left\\|\\tilde{\\mu}_n - \\mu\\right\\| \\to \\mathcal{N}\\left(0, \\frac{\\sigma_Y^2}{n}\\right)$$\n\n## Monte Carlo Confidence Intervals\n\nThe $\\alpha$-confidence interval is:\n$$\\tilde{\\mu}_n \\pm \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2}\\right) \\frac{\\sigma_Y}{\\sqrt{n}}$$\n\nFor example, the 95% confidence interval is $$\\tilde{\\mu}_n \\pm 1.96 \\frac{\\sigma_Y}{\\sqrt{n}}.$$\n\n## Implications of Monte Carlo Error\n\nConverging at a rate of $1/\\sqrt{n}$ is not great. But:\n\n- All models are wrong, and so there always exists some irreducible model error. \n- We often need a lot of simulations. Do we have enough computational power?\n\n## Implications of Monte Carlo Error\n\nIf you can compute your answer analytically, you probably should. \n\nBut often this is difficult if not impossible without many simplifying assumptions.\n\n## More Advanced Monte Carlo Methods\n\nThis type of \"simple\" Monte Carlo analysis assumes that we can readily sample independent and identically-distributed random variables. There are other methods for when distributions are hard to sample from or uncertainties aren't independent.\n\n## On Random Number Generators\n\n:::: {.columns}\n::: {.column width=40%}\nRandom number generators are not *really* random, only **pseudorandom**.\n\nThis is why setting a seed is important. But even that can go wrong...  \n:::\n::: {.column width=60%}\n\n![XKCD Cartoon 221: Random Number](https://imgs.xkcd.com/comics/random_number.png){width=90%}\n\n::: {.caption}\nSource: [XKCD #221](https://xkcd.com/221/)\n:::\n:::\n::::\n\n# Key Takeaways\n\n## Key Takeaways\n\n- Choice of probability distribution can have large impacts on uncertainty and risk estimates: **try not to use distributions just because they're convenient.**\n- **Monte Carlo**: Estimate expected values of functions using simulation.\n- Monte Carlo error is on the order $1/\\sqrt{n}$, so not great if more direct approaches are available and tractable.\n\n# Upcoming Schedule\n\n## Next Classes\n\n**Friday**: Simple Climate Models and Uncertainty\n\n**Next Week**: Prescriptive Models and Intro to Optimization\n\n",
    "supporting": [
      "lecture05-2_files"
    ],
    "filters": [],
    "includes": {}
  }
}