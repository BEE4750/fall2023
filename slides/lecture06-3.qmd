---
title: "Linear Programming"
subtitle: "Lecture 15"
author: "Vivek Srikrishnan"
course: "BEE 4750"
institution: "Cornell University"
date: "September 29, 2023"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        footer: "[BEE 4750, Environmental Systems Analysis](https://viveks.me/environmental-systems-analysis)"
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: mathjax
        include-in-header: mathjax-config.html
        date-format: long
        highlight-style: tango
        code-line-numbers: false
julia:
    exeflags: ["+1.10.4"]
execute:
    freeze: auto
---

```{julia}
#| echo: false
#| output: false

import Pkg
Pkg.activate(".")
Pkg.instantiate()
```

```{julia}
#| echo: false
#| output: false

using Measures
using Random
using Distributions
using Plots
using StatsPlots
using LaTeXStrings

Random.seed!(1)
```

# Review and Questions

## Last Class

- Optimization for solving decision problems.
- Discussed trial and error and search algorithms.
- For unconstrained problems, gradient-based methods can get "stuck" at local optima.
- Evolutionary algorithms can require many iterations and evaluations.
- Reviewed Lagrange multipliers for constrained optimization.

## Questions?

{{< include _poll-prompt.qmd >}}


# Linear Programming

## What is Linear Programming?

Linear programming refers to optimization for **linear models**.

As we will see over the next few weeks, linear models are:

- relatively simple (which lets us solve and analyze them "easily");
- surprisingly applicable to many contexts!

## Sidebar: Why "Programming"?

"Program" used to refer to military logistics, which is the origin of this field of research. For this historical reason, "mathematical programming" is often used instead of "mathematical optimization."

::: {.fragment .fade-in}
We typically restrict "program" for optimization problems which are formulated completely mathematically, versus we use a computer model to simulate the relationship between decision variables and outputs. 
:::

## Linear Functions

Recall that a function $f(x_1, \ldots, x_n)$ is **linear** if
$$f(x_1, \ldots, x_n) = a_1x_1 + a_2x_2 + \ldots + a_n x_n.$$

::: {.fragment .fade-in}
The key is that linear models are very simple geometrically:

- Define hyperplanes;
- Constant derivatives/gradients.
:::

## Linear Programs

A **linear program** (**LP**), or a *linear optimization model*, has the following characteristics:

- **Linearity**: The objective function and constraints are all linear;
- **Divisibility**: The decision variables are continuous (they can be fractional levels);
- **Certainty**: The problem is deterministic.

## Linearization

:::: {.columns}
::: {.column width=50%}
Linear models come up frequently because we can *linearize* nonlinear functions.

When we linearize components of an mathematical program, this is called the *linear relaxation* of the original problem.
:::
::: {.column width=50%}
```{julia}
E = 0:0.01:1
p = plot(E, E.^2, legend=false, grid=false, xlabel="Efficiency", ylabel="Cost", color=:black, yticks=false, xlims=(0, 1), ylims=(0, 1), left_margin=8mm, linewidth=3, tickfontsize=16, guidefontsize=16)
xticks!([0.65, 0.95])
xlims!((0, 1.05))
scatter!([0.65, 0.95], [0.65, 0.95].^2, markersize=10, color=:blue)
plot!(E, 1.6 .* E  .- 0.6175, color=:blue, linestyle=:dash, linewidth=3)
plot!(size=(600, 600))
```
:::
::::

## Linearization

:::: {.columns}
::: {.column width=50%}
Note that the linearization is often quite sensitive to the points that are used to find the slope.

So this is best done when you have a rough idea of the variable range of interest.
:::
::: {.column width=50%}
```{julia}
scatter!([0.32, 0.98], [0.32, 0.98].^2, markersize=10, color=:red)
plot!(p, E, 1.3 .* E  .- 0.31, color=:red, linestyle=:dot, linewidth=3)
```
:::
::::

## Why Is Solving LPs Straightforward?

:::: {.columns}
::: {.column width=50%}
```{julia, lp-sketch}
x = 2:0.1:11
f1(x) = 4.5 .* x
f2(x) = -x .+ 16
f3(x) = -1.5 .* x .+ 12
f4(x) = 0.5 .* x

p = plot(x, max.(f3(x), f4(x)), fillrange=min.(f1(x), f2(x)), color=:lightblue, grid=true, legend=false, xlabel=L"x", ylabel=L"y", xlims=(-2, 20), framestyle=:origin, ylims=(-2, 20), minorticks=5, tickfontsize=16, guidefontsize=16, right_margin=5mm)
plot!(-2:0.1:20, f1.(-2:0.1:20), color=:green, linewidth=3)
plot!(-2:0.1:20, f2.(-2:0.1:20), color=:red, linewidth=3)
plot!(-2:0.1:20, f3.(-2:0.1:20), color=:brown, linewidth=3)
plot!(-2:0.1:20, f4.(-2:0.1:20), color=:purple, linewidth=3)
plot!(size=(600, 600))
```
:::
::: {.column width=50%}
All solutions must exist on the boundary of the feasible region (which must be a *polytope*).
:::
::::

## Why Is Solving LPs Straightforward?

:::: {.columns}
::: {.column width=50%}
```{julia}
p
```
:::
::: {.column width=50%}
More specifically:

::: {.incremental}
- an optimum solution must occur at the intersection of constraints;
- can focus on finding and analyzing the corners. 
:::
:::
::::

## Why Is Solving LPs Straightforward?

:::: {.columns}
::: {.column width=50%}
```{julia}
p
```
:::
::: {.column width=50%}
This is the basis of all *simplex* methods for solving LPs.

These methods go back to George Dantzig in the 1940s and are still widely used today.
:::
::::

## Why Do LPs Have A Corner Solution?

:::: {.columns}
::: {.column width=50%}

```{julia}
x = 2:0.1:11
f1(x) = 4.5 .* x
f2(x) = -x .+ 16
f3(x) = -1.5 .* x .+ 12
f4(x) = 0.5 .* x

g(y1, y2) = 5 * y1 + 2 * y2
z = @. g((1:.01:12)', 1:.01:15)

p2 = contour(1:.01:12, 1:.01:15, z, c=:YlOrRd, grid=true, legend=false, xlabel=L"x", ylabel=L"y", xlims=(-2, 20), framestyle=:origin, ylims=(-2, 20), minorticks=5, tickfontsize=16, guidefontsize=16, right_margin=5mm)
plot!(-2:0.1:20, f1.(-2:0.1:20), color=:green, linewidth=3)
plot!(-2:0.1:20, f2.(-2:0.1:20), color=:red, linewidth=3)
plot!(-2:0.1:20, f3.(-2:0.1:20), color=:brown, linewidth=3)
plot!(-2:0.1:20, f4.(-2:0.1:20), color=:purple, linewidth=3)
plot!(size=(600, 600))
```
:::
::: {.column width=50%}
Can a solution be in the interior?
:::
::::

## Why Do LPs Have A Corner Solution?

:::: {.columns}
::: {.column width=50%}

```{julia}
plot!(p2, [5, 7.5], [6, 7], arrow=true, color=:black, linewidth=3)
```

:::
::: {.column width=50%}
::: {.fragment .fade-in}
What about along an edge but not a corner?
:::
:::
::::

## Example: Solving an LP

:::: {.columns}
::: {.column width=50%}
$$\begin{alignedat}{3}
& \max_{x_1, x_2} &  230x_1 + 120x_2 &  \\\\
& \text{subject to:} & &\\
& & 0.9x_1 + 0.5x_2 &\leq 600 \\
& & x_1 + x_2 &\leq 1000 \\
& & x_1, x_2 &\geq 0
\end{alignedat}$$
:::
::: {.column width=50%}
```{julia}
x1 = 0:1200
x2 = 0:1400
f1(x) = (600 .- 0.9 .* x) ./ 0.5
f2(x) = 1000 .- x

p = plot(0:667, min.(f1(0:667), f2(0:667)), fillrange=0, color=:lightblue, grid=true, label="Feasible Region", xlabel=L"x_1", ylabel=L"x_2", xlims=(-50, 1200), ylims=(-50, 1400), framestyle=:origin, minorticks=4, right_margin=4mm, left_margin=4mm, legendfontsize=14, tickfontsize=16, guidefontsize=16)
plot!(0:667, f1.(0:667), color=:brown, linewidth=3, label=false)
plot!(0:1000, f2.(0:1000), color=:red, linewidth=3, label=false)
annotate!(400, 1100, text(L"0.9x_1 + 0.5x_2 = 600", color=:purple, pointsize=18))
annotate!(1000, 300, text(L"x_1 + x_2 = 1000", color=:red, pointsize=18))
plot!(size=(600, 600))
```
:::
::::

## Example: Solving an LP

:::: {.columns}
::: {.column width=50%}
$$\begin{alignedat}{3}
& \max_{x_1, x_2} &  230x_1 + 120x_2 &  \\\\
& \text{subject to:} & &\\
& & 0.9x_1 + 0.5x_2 &\leq 600 \\
& & x_1 + x_2 &\leq 1000 \\
& & x_1, x_2 &\geq 0
\end{alignedat}$$
:::
::: {.column width=50%}
```{julia}
Z(x1,x2) = 230 * x1 + 120 * x2
contour!(0:660,0:1000,(x1,x2)->Z(x1,x2), levels=5, c=:devon, linewidth=2, colorbar = false, clabels = true) 
```
:::
::::

## Examining Corner Points

:::: {.columns}
::: {.column width=50%}
Corner $(x_1, x_2)$ | Objective
:------------: | -------------:
$(0,0)$      | $0$
$(0, 1000)$ | $12000$
$(667, 0)$  | $153410$
$(250, 750)$ | $147500$
:::
::: {.column width=50%}

```{julia}
scatter!(p, [0, 0, 667, 250], [0, 1000, 0, 750], markersize=10, z=2, label="Corner Point", markercolor=:orange)
```
:::
::::

## Solving an LP

Manually checking the corner points is all well and good for this simple example, but does it scale well?

::: {.fragment .fade-in}
LP solvers (often based off the simplex method) automate this process.
:::

# Shadow Prices

## Binding Constraints

A solution will be found at one of the corner points of the feasible polytope.

This means that at this solution, one or more constraints are **binding**: if we *relaxed* the constraint by weakening it, we could improve the solution.

## Binding Constraints

:::: {.columns}
::: {.column width=50%}
```{julia}
p
```
:::

::: {.column width=50%}
The binding constraints are 

- $x_1 \geq 0$ 
- $0.9 x_1 + 0.5 x_2 \leq 600$, 

but not 

- $x_1 + x_2 \leq 1000$.
:::
::::

## Shadow Prices

The **marginal cost** of a constraint is the amount by which the solution would improve if the constraint capacity was relaxed by one unit.

This is also referred to as the **shadow price** (these are also called the *dual variables* of the constraint).

Non-zero shadow prices tell us that the constraint is binding, and their values rank which constraints are most influential.

## Shadow Prices are Lagrange Multipliers

The shadow prices are the Lagrange Multipliers of the optimization problem.

If our inequality constraints $X \geq A$ and $X \leq B$ are written as $X - S_1^2 = A$ and $X + S_2^2 = B$:

$$H(X, S_1 S_2, \lambda_1, \lambda_2) = Z(X) - \lambda_1(X - S_1^2 - A) - \lambda_2(X + S_2^2 - B),$$

$$\Rightarrow \qquad \frac{\partial H}{\partial A} = \lambda_1, \qquad \frac{\partial H}{\partial B} = \lambda_2.$$


# Key Takeaways

## Key Takeaways

- Linear Programs: straightforward to solve!
- For an LP, an optimum must occur at a corner of the feasible polytope.
- Shadow prices (dual variables) of the constraints are the rate by which the solution would improve if the constraints were relaxed.

# Upcoming Schedule

## Next Classes

**Monday**: Project proposal peer review

**Wednesday**: Guest lecture by Diana Hackett (Mann librarian) on regulations research.

**Friday**: Lab on linear programming in Julia.
